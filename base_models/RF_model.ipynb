{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/stasvlad/Documents/hse/sberbank\n"
     ]
    }
   ],
   "source": [
    "%cd /home/stasvlad/Documents/hse/sberbank/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2TDIoXTN-wtp"
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from features import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBRegressor, DMatrix, cv\n",
    "from xgboost import train as train_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70sx-8YD-wtx"
   },
   "source": [
    "## Data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lgqwWeGS-wtz",
    "outputId": "7ce4db16-583d-40af-afa9-a00f6aebc45c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix:  550\n",
      "Fix:  149\n"
     ]
    }
   ],
   "source": [
    "macro_df = pd.read_csv('data/macro.csv', parse_dates=['timestamp'])\n",
    "train_df = pd.read_csv('data/train.csv', index_col='id', parse_dates=['timestamp'])\n",
    "test_df = pd.read_csv('data/test.csv', index_col='id', parse_dates=['timestamp'])\n",
    "\n",
    "tverskoe_issue_fix(train_df)\n",
    "tverskoe_issue_fix(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XMfRzcU-wt2"
   },
   "source": [
    "## 1. Data preprocessing\n",
    "## I part (encoding and correcting mistakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlOmdlGc-wt4"
   },
   "source": [
    "### Macro dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a1-RKD2-wt4",
    "outputId": "364f5c9a-8412-44e4-8716-0bb10c3dd98d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "macro_df['child_on_acc_pre_school'] = macro_df['child_on_acc_pre_school'].str.replace('#!', 'nan')\n",
    "for column in macro_df.select_dtypes('object').columns:\n",
    "    macro_df[column] = macro_df[column].str.replace(',', '.')\n",
    "    macro_df[column] = macro_df[column].astype(float)\n",
    "\n",
    "if not len(macro_df.select_dtypes('object').columns):\n",
    "    print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hOdB2O2-wt5"
   },
   "source": [
    "### Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cefhW3sO-wt5"
   },
   "outputs": [],
   "source": [
    "train_df = encode(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSRp5y_C-wt7"
   },
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IMeNQ_Ad-wt8"
   },
   "outputs": [],
   "source": [
    "test_df = encode(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcRHlWsU-wt-"
   },
   "source": [
    "## II part (Filling missing values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8kHIiU0-wt_"
   },
   "source": [
    "XGBRegressor model handles `np.NaN` values itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfLaI5aH-wt_"
   },
   "source": [
    "## 2. Encoding `sub_area` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_train_df = pd.read_csv('data/geo/train_lat_lon.csv')\n",
    "coords_train_df.drop(['key', 'tolerance_m'], axis=1, inplace=True)\n",
    "coords_train_df.index = coords_train_df.id\n",
    "coords_train_df.drop(['id'], axis=1, inplace=True)\n",
    "coords_train_df = coords_train_df.sort_index()\n",
    "\n",
    "coords_test_df = pd.read_csv('data/geo/test_lat_lon.csv')\n",
    "coords_test_df.drop(['key', 'tolerance_m'], axis=1, inplace=True)\n",
    "coords_test_df.index = coords_test_df.id\n",
    "coords_test_df.drop(['id'], axis=1, inplace=True)\n",
    "coords_test_df = coords_test_df.sort_index()\n",
    "\n",
    "coords_all_df = pd.concat([coords_train_df, coords_test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['is_train'] = 1\n",
    "test_df['is_train'] = 0\n",
    "\n",
    "# coords_df = pd.read_csv('data/coords.csv', index_col='id')\n",
    "all_df = pd.concat([train_df, test_df])\n",
    "\n",
    "all_df['latitude'] = coords_all_df['lat']\n",
    "all_df['longitude'] = coords_all_df['lon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ad9TaN9Q-wuB"
   },
   "source": [
    "## 3. Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GS7abrdJ-wuB"
   },
   "outputs": [],
   "source": [
    "all_df = remove_outliers(all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Y7doThf-wuB"
   },
   "source": [
    "## 4. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HEFaFZFI-wuC"
   },
   "outputs": [],
   "source": [
    "all_df = create_new_features(all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLOjaYTj-wuD"
   },
   "source": [
    "## 5. Removing fake prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2gVmPZwh-wuE"
   },
   "outputs": [],
   "source": [
    "train_df = all_df[all_df['is_train'] == 1].drop(['is_train'], axis=1)\n",
    "test_df = all_df[all_df['is_train'] == 0].drop(['is_train', 'price_doc'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVED: 35\n"
     ]
    }
   ],
   "source": [
    "train_df = remove_fake_prices(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "r2iMQl-I-wuE"
   },
   "outputs": [],
   "source": [
    "idx_outliers = np.loadtxt('outliers/idx_outliers_full.txt').astype(int)\n",
    "train_df = train_df.drop(idx_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(['price_doc', 'sub_area'], axis=1)\n",
    "y = np.log1p(train_df['price_doc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleImputer(strategy='median')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = pd.concat([train_df.drop(['sub_area', 'price_doc'], axis=1), test_df.drop('sub_area', axis=1)])\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   23.6s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   25.3s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   28.1s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   28.2s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   28.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  6.6min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  6.6min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  6.7min remaining: 10.0min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  6.7min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  6.7min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  6.7min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  6.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.19347543, -0.18977763, -0.19218449, -0.18735222, -0.19587987])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = imputer.transform(X)\n",
    "RF = RandomForestRegressor(n_estimators=500, max_depth=5, max_features=0.5, verbose=True, n_jobs=-1)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cross_val_score(RF, X, y, cv=kf, scoring='neg_root_mean_squared_error', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "kf_split = list(kf.split(X, y))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   44.9s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=10, max_features=0.5, min_samples_leaf=10,\n",
       "                      n_estimators=500, n_jobs=-1, verbose=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = imputer.transform(X)\n",
    "RF = RandomForestRegressor(n_estimators=500, max_depth=10, min_samples_leaf=10, max_features=0.5, n_jobs=-1)\n",
    "RF.fit(X[kf_split[0]], y.iloc[kf_split[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15301050714337347"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(RF.predict(X[kf_split[1]]), y.iloc[kf_split[1]], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=8, max_features=0.5, min_samples_leaf=10,\n",
       "                      n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = imputer.transform(X)\n",
    "RF = RandomForestRegressor(n_estimators=500, max_depth=8, min_samples_leaf=10, max_features=0.5, n_jobs=-1)\n",
    "RF.fit(X[kf_split[0]], y.iloc[kf_split[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1628513351366661"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(RF.predict(X[kf_split[1]]), y.iloc[kf_split[1]], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=12, max_features=0.5, n_estimators=500,\n",
       "                      n_jobs=-1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = imputer.transform(X)\n",
    "RF = RandomForestRegressor(n_estimators=500, max_depth=12, max_features=0.5, n_jobs=-1)\n",
    "RF.fit(X[kf_split[0]], y.iloc[kf_split[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1440972771109638"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(RF.predict(X[kf_split[1]]), y.iloc[kf_split[1]], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features=0.5, n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = imputer.transform(X)\n",
    "RF = RandomForestRegressor(n_estimators=500, max_features=0.5, n_jobs=-1)\n",
    "RF.fit(X[kf_split[0]], y.iloc[kf_split[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13782184458797508"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(RF.predict(X[kf_split[1]]), y.iloc[kf_split[1]], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features=0.3, n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = imputer.transform(X)\n",
    "RF = RandomForestRegressor(n_estimators=500, max_features=0.3, n_jobs=-1)\n",
    "RF.fit(X[kf_split[0]], y.iloc[kf_split[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13854546750120925"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(RF.predict(X[kf_split[1]]), y.iloc[kf_split[1]], squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF = RandomForestRegressor(n_estimators=500, max_features=0.5, n_jobs=-1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
