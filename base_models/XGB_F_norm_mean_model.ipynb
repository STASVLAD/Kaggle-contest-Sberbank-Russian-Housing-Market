{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/stasvlad/Documents/hse/sberbank\n"
     ]
    }
   ],
   "source": [
    "%cd /home/stasvlad/Documents/hse/sberbank/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2TDIoXTN-wtp"
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from features import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBRegressor, DMatrix, cv\n",
    "from xgboost import train as train_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70sx-8YD-wtx"
   },
   "source": [
    "## Data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lgqwWeGS-wtz",
    "outputId": "7ce4db16-583d-40af-afa9-a00f6aebc45c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix:  550\n",
      "Fix:  149\n"
     ]
    }
   ],
   "source": [
    "macro_df = pd.read_csv('data/macro.csv', parse_dates=['timestamp'])\n",
    "train_df = pd.read_csv('data/train.csv', index_col='id', parse_dates=['timestamp'])\n",
    "test_df = pd.read_csv('data/test.csv', index_col='id', parse_dates=['timestamp'])\n",
    "\n",
    "tverskoe_issue_fix(train_df)\n",
    "tverskoe_issue_fix(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XMfRzcU-wt2"
   },
   "source": [
    "## 1. Data preprocessing\n",
    "## I part (encoding and correcting mistakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlOmdlGc-wt4"
   },
   "source": [
    "### Macro dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a1-RKD2-wt4",
    "outputId": "364f5c9a-8412-44e4-8716-0bb10c3dd98d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "macro_df['child_on_acc_pre_school'] = macro_df['child_on_acc_pre_school'].str.replace('#!', 'nan')\n",
    "for column in macro_df.select_dtypes('object').columns:\n",
    "    macro_df[column] = macro_df[column].str.replace(',', '.')\n",
    "    macro_df[column] = macro_df[column].astype(float)\n",
    "\n",
    "if not len(macro_df.select_dtypes('object').columns):\n",
    "    print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hOdB2O2-wt5"
   },
   "source": [
    "### Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cefhW3sO-wt5"
   },
   "outputs": [],
   "source": [
    "train_df = encode(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSRp5y_C-wt7"
   },
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IMeNQ_Ad-wt8"
   },
   "outputs": [],
   "source": [
    "test_df = encode(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcRHlWsU-wt-"
   },
   "source": [
    "## II part (Filling missing values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8kHIiU0-wt_"
   },
   "source": [
    "XGBRegressor model handles `np.NaN` values itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfLaI5aH-wt_"
   },
   "source": [
    "## 2. Encoding `sub_area` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_train_df = pd.read_csv('data/geo/train_lat_lon.csv')\n",
    "coords_train_df.drop(['key', 'tolerance_m'], axis=1, inplace=True)\n",
    "coords_train_df.index = coords_train_df.id\n",
    "coords_train_df.drop(['id'], axis=1, inplace=True)\n",
    "coords_train_df = coords_train_df.sort_index()\n",
    "\n",
    "coords_test_df = pd.read_csv('data/geo/test_lat_lon.csv')\n",
    "coords_test_df.drop(['key', 'tolerance_m'], axis=1, inplace=True)\n",
    "coords_test_df.index = coords_test_df.id\n",
    "coords_test_df.drop(['id'], axis=1, inplace=True)\n",
    "coords_test_df = coords_test_df.sort_index()\n",
    "\n",
    "coords_all_df = pd.concat([coords_train_df, coords_test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['is_train'] = 1\n",
    "test_df['is_train'] = 0\n",
    "\n",
    "# coords_df = pd.read_csv('data/coords.csv', index_col='id')\n",
    "all_df = pd.concat([train_df, test_df])\n",
    "\n",
    "all_df['latitude'] = coords_all_df['lat']\n",
    "all_df['longitude'] = coords_all_df['lon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ad9TaN9Q-wuB"
   },
   "source": [
    "## 3. Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GS7abrdJ-wuB"
   },
   "outputs": [],
   "source": [
    "all_df = remove_outliers(all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Y7doThf-wuB"
   },
   "source": [
    "## 4. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HEFaFZFI-wuC"
   },
   "outputs": [],
   "source": [
    "all_df = create_new_features(all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLOjaYTj-wuD"
   },
   "source": [
    "## 5. Removing fake prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2gVmPZwh-wuE"
   },
   "outputs": [],
   "source": [
    "train_df = all_df[all_df['is_train'] == 1].drop(['is_train'], axis=1)\n",
    "test_df = all_df[all_df['is_train'] == 0].drop(['is_train', 'price_doc'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVED: 35\n"
     ]
    }
   ],
   "source": [
    "train_df = remove_fake_prices(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "r2iMQl-I-wuE"
   },
   "outputs": [],
   "source": [
    "idx_outliers = np.loadtxt('outliers/idx_outliers_full.txt').astype(int)\n",
    "train_df = train_df.drop(idx_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "kfljzjWx-wuF"
   },
   "outputs": [],
   "source": [
    "class my_LGBRegressor(object):\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def fit(self, X, y, w=None):\n",
    "        split = int(X.shape[0] * 0.8)\n",
    "        indices = np.random.permutation(X.shape[0])\n",
    "        train_id, test_id = indices[:split], indices[split:]\n",
    "        X_train, y_train, w_train, X_val, y_val, w_val = X[train_id], y[train_id], w[train_id], X[test_id], y[test_id], w[test_id],\n",
    "        d_train = lgb.Dataset(X_train, y_train, weight=w_train)\n",
    "        d_valid = lgb.Dataset(X_val, y_val, weight=w_val) \n",
    "\n",
    "        bst_partial = lgb.train(self.params,\n",
    "                                d_train, 10000,\n",
    "                                valid_sets=d_valid,\n",
    "                                callbacks = [lgb.early_stopping(50)])\n",
    "                                \n",
    "        num_round = bst_partial.best_iteration\n",
    "        d_all = lgb.Dataset(X, label=y, weight=w)\n",
    "        self.bst = lgb.train(self.params, d_all, num_round)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.bst.predict(X)\n",
    "\n",
    "\n",
    "class my_XGBRegressor(object):\n",
    "    def __init__(self, params, product_type=-1):\n",
    "        self.params = params\n",
    "        self.product_type = product_type\n",
    "\n",
    "    def fit(self, X, y, w=None):\n",
    "        \n",
    "        if self.product_type == 0:\n",
    "            X = train_df[train_df['product_type'] == 0].drop(['sub_area', 'price_doc'], axis=1).values\n",
    "            y = np.log1p(test_df[test_df['product_type'] == 0]['price_doc'].values)\n",
    "            print(X.shape)\n",
    "\n",
    "        if self.product_type == 1:\n",
    "            X = train_df[train_df['product_type'] == 1].drop(['sub_area', 'price_doc'], axis=1).values\n",
    "            y = np.log1p(test_df[test_df['product_type'] == 1]['price_doc'].values)\n",
    "            print(X.shape)\n",
    "            \n",
    "        split = int(X.shape[0] * 0.8)\n",
    "        indices = np.random.permutation(X.shape[0])\n",
    "        train_id, test_id = indices[:split], indices[split:]\n",
    "\n",
    "        X_train, y_train, w_train, X_val, y_val, w_val = X[train_id], y[train_id], w[train_id], X[test_id], y[test_id], w[test_id],\n",
    "        X_train, y_train, w_train, X_val, y_val, w_val = X[train_id], y[train_id], w[train_id], X[test_id], y[test_id], w[test_id],\n",
    "\n",
    "        d_train = DMatrix(X_train, label=y_train, weight = w_train)\n",
    "        d_valid = DMatrix(X_val, label=y_val, weight = w_val) \n",
    "\n",
    "        print(f\"Training until validation scores don't improve for 50 rounds\") # !!!\n",
    "        if self.params['booster'] == 'gblinear':\n",
    "            num_boost_round = 10000\n",
    "        else:\n",
    "            num_boost_round = 5000\n",
    "\n",
    "        bst_partial = train_xgb(self.params,\n",
    "                                d_train,\n",
    "                                num_boost_round=num_boost_round,\n",
    "                                early_stopping_rounds=50,\n",
    "                                evals=[(d_train, 'train'), (d_valid, 'val')],\n",
    "                                verbose_eval=500)\n",
    "\n",
    "        last_round = bst_partial.best_iteration\n",
    "        print(f\"[{last_round}]  RMSE: {bst_partial.best_score}\")\n",
    "\n",
    "        d_all = DMatrix(X, label=y, weight = w)\n",
    "        self.bst = train_xgb(self.params,\n",
    "                             d_all,\n",
    "                             num_boost_round=last_round,\n",
    "                             evals=[(d_train, 'train')],\n",
    "                             verbose_eval=500)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        d_test = DMatrix(X_test)\n",
    "        return self.bst.predict(d_test)\n",
    "\n",
    "\n",
    "class Ensemble(object):\n",
    "    def __init__(self, n_folds, stacker, base_models):\n",
    "        self.n_folds = n_folds\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit_predict(self, train_df, test_df):\n",
    "        w = train_df['w'].values\n",
    "        X = train_df.drop(['sub_area', 'price_doc', 'w'], axis=1).values\n",
    "        y = np.log1p(train_df['price_doc']).values\n",
    "        X_test = test_df.drop('sub_area', axis=1).values\n",
    "\n",
    "        all_df = pd.concat([train_df.drop(['sub_area', 'price_doc', 'w'], axis=1), test_df.drop('sub_area', axis=1)])\n",
    "        imputer = SimpleImputer(strategy='median') # mean\n",
    "        imputer.fit(all_df)\n",
    "\n",
    "        kf = KFold(n_splits=self.n_folds, shuffle=True)  # random_state=42\n",
    "        folds = list(kf.split(X, y))\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test = np.zeros((X_test.shape[0], len(self.base_models)))\n",
    "\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            print('\\n\\nTraining model: ' + str(type(model).__name__))\n",
    "            S_test_i = np.zeros((X_test.shape[0], len(folds)))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                print('ROUND ' + str(j+1))\n",
    "\n",
    "                if (not isinstance(model, my_XGBRegressor)) and (not isinstance(model, my_LGBRegressor)):\n",
    "                    X = imputer.transform(train_df.drop(['sub_area', 'price_doc', 'w'], axis=1).values)\n",
    "                    X_test = imputer.transform(X_test)\n",
    "\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                w_train = w[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                y_holdout = y[test_idx]\n",
    "\n",
    "                model.fit(X_train, y_train, w_train)\n",
    "\n",
    "                y_train_pred = model.predict(X_train)\n",
    "                y_pred = model.predict(X_holdout)\n",
    "\n",
    "                print(f\"[ALL]  train-RMSE  : {mean_squared_error(y_train_pred, y_train, squared=False)}\")\n",
    "                print(f\"[ALL]  holdout-RMSE: {mean_squared_error(y_pred, y_holdout, squared=False)}\")\n",
    "\n",
    "                S_train[test_idx, i] = y_pred\n",
    "                S_test_i[:, j] = model.predict(X_test)\n",
    "\n",
    "            S_test[:, i] = S_test_i.mean(axis=1)\n",
    "\n",
    "        self.S_train, self.S_test, self.y = S_train, S_test, y\n",
    "        self.stacker.fit(S_train, y)\n",
    "        y_pred = self.stacker.predict(S_test)\n",
    "        y_pred_train = self.stacker.predict(S_train)\n",
    "        print(f\"\\n\\n[THE END]  train-RMSE  : {mean_squared_error(y_pred_train, y, squared=False)}\")\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['w'] = 1\n",
    "train_df.loc[train_df['timestamp_year'] == 2014, 'w'] = 1.2\n",
    "train_df.loc[train_df['timestamp_year'] == 2015, 'w'] = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Normalizing prices (`product_type == 'Investment'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_0 = train_df[train_df['product_type'] == 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2011_q3_index = train_df_0.loc[train_df_0['timestamp_year'] == 2011].loc[train_df_0['timestamp_month'] >= 7].loc[train_df_0['timestamp_month'] < 10].index\n",
    "train_2011_q4_index = train_df_0.loc[train_df_0['timestamp_year'] == 2011].loc[train_df_0['timestamp_month'] >= 10].loc[train_df_0['timestamp_month'] <= 12].index\n",
    "train_2012_q1_index = train_df_0.loc[train_df_0['timestamp_year'] == 2012].loc[train_df_0['timestamp_month'] >= 1].loc[train_df_0['timestamp_month'] < 4].index\n",
    "train_2012_q2_index = train_df_0.loc[train_df_0['timestamp_year'] == 2012].loc[train_df_0['timestamp_month'] >= 4].loc[train_df_0['timestamp_month'] < 7].index\n",
    "train_2012_q3_index = train_df_0.loc[train_df_0['timestamp_year'] == 2012].loc[train_df_0['timestamp_month'] >= 7].loc[train_df_0['timestamp_month'] < 10].index\n",
    "train_2012_q4_index = train_df_0.loc[train_df_0['timestamp_year'] == 2012].loc[train_df_0['timestamp_month'] >= 10].loc[train_df_0['timestamp_month'] <= 12].index\n",
    "train_2013_q1_index = train_df_0.loc[train_df_0['timestamp_year'] == 2013].loc[train_df_0['timestamp_month'] >= 1].loc[train_df_0['timestamp_month'] < 4].index\n",
    "train_2013_q2_index = train_df_0.loc[train_df_0['timestamp_year'] == 2013].loc[train_df_0['timestamp_month'] >= 4].loc[train_df_0['timestamp_month'] < 7].index\n",
    "train_2013_q3_index = train_df_0.loc[train_df_0['timestamp_year'] == 2013].loc[train_df_0['timestamp_month'] >= 7].loc[train_df_0['timestamp_month'] < 10].index\n",
    "train_2013_q4_index = train_df_0.loc[train_df_0['timestamp_year'] == 2013].loc[train_df_0['timestamp_month'] >= 10].loc[train_df_0['timestamp_month'] <= 12].index\n",
    "train_2014_q1_index = train_df_0.loc[train_df_0['timestamp_year'] == 2014].loc[train_df_0['timestamp_month'] >= 1].loc[train_df_0['timestamp_month'] < 4].index\n",
    "train_2014_q2_index = train_df_0.loc[train_df_0['timestamp_year'] == 2014].loc[train_df_0['timestamp_month'] >= 4].loc[train_df_0['timestamp_month'] < 7].index\n",
    "train_2014_q3_index = train_df_0.loc[train_df_0['timestamp_year'] == 2014].loc[train_df_0['timestamp_month'] >= 7].loc[train_df_0['timestamp_month'] < 10].index\n",
    "train_2014_q4_index = train_df_0.loc[train_df_0['timestamp_year'] == 2014].loc[train_df_0['timestamp_month'] >= 10].loc[train_df_0['timestamp_month'] <= 12].index\n",
    "train_2015_q1_index = train_df_0.loc[train_df_0['timestamp_year'] == 2015].loc[train_df_0['timestamp_month'] >= 1].loc[train_df_0['timestamp_month'] < 4].index\n",
    "train_2015_q2_index = train_df_0.loc[train_df_0['timestamp_year'] == 2015].loc[train_df_0['timestamp_month'] >= 4].loc[train_df_0['timestamp_month'] < 7].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q_idx = [train_2011_q3_index, train_2011_q4_index, train_2012_q1_index,\n",
    "               train_2012_q2_index, train_2012_q3_index, train_2012_q4_index,\n",
    "               train_2013_q1_index, train_2013_q2_index, train_2013_q3_index,\n",
    "               train_2013_q4_index, train_2014_q1_index, train_2014_q2_index,\n",
    "               train_2014_q3_index, train_2014_q4_index, train_2015_q1_index,\n",
    "               train_2015_q2_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(q) for q in train_q_idx) == len(train_df_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([28765, 28771, 28772, 28774, 28775, 28777, 28778, 28781, 28783,\n",
       "            28784,\n",
       "            ...\n",
       "            30455, 30457, 30460, 30463, 30464, 30466, 30469, 30470, 30472,\n",
       "            30473],\n",
       "           dtype='int64', name='id', length=716)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_q_idx.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(q) for q in train_q_idx) == len(train_df_0) - len(train_df.loc[train_2015_q2_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146003.66978784843\n",
      "147007.56866771728\n",
      "154123.60472626315\n",
      "156463.75911237008\n",
      "157370.6808427691\n",
      "158166.67377496415\n",
      "160119.11724372697\n",
      "159137.1708521332\n",
      "163726.89049531604\n",
      "158594.24925726495\n",
      "161002.8972664781\n",
      "165221.11492517497\n",
      "165168.61430014294\n",
      "169940.21707769358\n",
      "173190.639734523\n"
     ]
    }
   ],
   "source": [
    "for q in train_q_idx:\n",
    "    print((train_df.loc[q].price_doc / train_df.loc[q].full_sq).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['average_q_price'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_2015_q2_index, 'average_q_price'] = 1\n",
    "base_price = (train_df.loc[train_2015_q2_index].price_doc / train_df.loc[train_2015_q2_index].full_sq).mean()\n",
    "for q in train_q_idx:\n",
    "    train_df.loc[q, 'average_q_price'] = base_price / (train_df.loc[q].price_doc / train_df.loc[q].full_sq).mean() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Normalizing prices (`product_type == 'OwnerOccupied'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1 = train_df[train_df['product_type'] == 1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2011_q3_index = train_df_1.loc[train_df_1['timestamp_year'] == 2011].loc[train_df_1['timestamp_month'] >= 7].loc[train_df_1['timestamp_month'] < 10].index\n",
    "train_2011_q4_index = train_df_1.loc[train_df_1['timestamp_year'] == 2011].loc[train_df_1['timestamp_month'] >= 10].loc[train_df_1['timestamp_month'] <= 12].index\n",
    "train_2012_q1_index = train_df_1.loc[train_df_1['timestamp_year'] == 2012].loc[train_df_1['timestamp_month'] >= 1].loc[train_df_1['timestamp_month'] < 4].index\n",
    "train_2012_q2_index = train_df_1.loc[train_df_1['timestamp_year'] == 2012].loc[train_df_1['timestamp_month'] >= 4].loc[train_df_1['timestamp_month'] < 7].index\n",
    "train_2012_q3_index = train_df_1.loc[train_df_1['timestamp_year'] == 2012].loc[train_df_1['timestamp_month'] >= 7].loc[train_df_1['timestamp_month'] < 10].index\n",
    "train_2012_q4_index = train_df_1.loc[train_df_1['timestamp_year'] == 2012].loc[train_df_1['timestamp_month'] >= 10].loc[train_df_1['timestamp_month'] <= 12].index\n",
    "train_2013_q1_index = train_df_1.loc[train_df_1['timestamp_year'] == 2013].loc[train_df_1['timestamp_month'] >= 1].loc[train_df_1['timestamp_month'] < 4].index\n",
    "train_2013_q2_index = train_df_1.loc[train_df_1['timestamp_year'] == 2013].loc[train_df_1['timestamp_month'] >= 4].loc[train_df_1['timestamp_month'] < 7].index\n",
    "train_2013_q3_index = train_df_1.loc[train_df_1['timestamp_year'] == 2013].loc[train_df_1['timestamp_month'] >= 7].loc[train_df_1['timestamp_month'] < 10].index\n",
    "train_2013_q4_index = train_df_1.loc[train_df_1['timestamp_year'] == 2013].loc[train_df_1['timestamp_month'] >= 10].loc[train_df_1['timestamp_month'] <= 12].index\n",
    "train_2014_q1_index = train_df_1.loc[train_df_1['timestamp_year'] == 2014].loc[train_df_1['timestamp_month'] >= 1].loc[train_df_1['timestamp_month'] < 4].index\n",
    "train_2014_q2_index = train_df_1.loc[train_df_1['timestamp_year'] == 2014].loc[train_df_1['timestamp_month'] >= 4].loc[train_df_1['timestamp_month'] < 7].index\n",
    "train_2014_q3_index = train_df_1.loc[train_df_1['timestamp_year'] == 2014].loc[train_df_1['timestamp_month'] >= 7].loc[train_df_1['timestamp_month'] < 10].index\n",
    "train_2014_q4_index = train_df_1.loc[train_df_1['timestamp_year'] == 2014].loc[train_df_1['timestamp_month'] >= 10].loc[train_df_1['timestamp_month'] <= 12].index\n",
    "train_2015_q1_index = train_df_1.loc[train_df_1['timestamp_year'] == 2015].loc[train_df_1['timestamp_month'] >= 1].loc[train_df_1['timestamp_month'] < 4].index\n",
    "train_2015_q2_index = train_df_1.loc[train_df_1['timestamp_year'] == 2015].loc[train_df_1['timestamp_month'] >= 4].loc[train_df_1['timestamp_month'] < 7].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q_idx = [train_2011_q3_index, train_2011_q4_index, train_2012_q1_index,\n",
    "               train_2012_q2_index, train_2012_q3_index, train_2012_q4_index,\n",
    "               train_2013_q1_index, train_2013_q2_index, train_2013_q3_index,\n",
    "               train_2013_q4_index, train_2014_q1_index, train_2014_q2_index,\n",
    "               train_2014_q3_index, train_2014_q4_index, train_2015_q1_index,\n",
    "               train_2015_q2_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(q) for q in train_q_idx) == len(train_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([28763, 28764, 28766, 28767, 28768, 28770, 28773, 28776, 28779,\n",
       "            28780,\n",
       "            ...\n",
       "            30453, 30454, 30456, 30458, 30461, 30462, 30465, 30467, 30468,\n",
       "            30471],\n",
       "           dtype='int64', name='id', length=896)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_q_idx.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(q) for q in train_q_idx) == len(train_df_1) - len(train_df.loc[train_2015_q2_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113073.97260273973\n",
      "136145.3975765336\n",
      "147778.72739137296\n",
      "138993.9483576403\n",
      "96237.5923084805\n",
      "90742.56271618714\n",
      "98639.39428290667\n",
      "99585.48477661972\n",
      "104655.732082085\n",
      "107174.24678825888\n",
      "106913.52490299725\n",
      "111861.94168330009\n",
      "115122.04022919902\n",
      "118082.13624021263\n",
      "120242.83070880704\n"
     ]
    }
   ],
   "source": [
    "for q in train_q_idx:\n",
    "    print((train_df.loc[q].price_doc / train_df.loc[q].full_sq).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_2015_q2_index, 'average_q_price'] = 1\n",
    "base_price = (train_df.loc[train_2015_q2_index].price_doc / train_df.loc[train_2015_q2_index].full_sq).mean()\n",
    "for q in train_q_idx:\n",
    "    train_df.loc[q, 'average_q_price'] = base_price / (train_df.loc[q].price_doc / train_df.loc[q].full_sq).mean() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['price_doc'] = train_df['price_doc'] * train_df['average_q_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'objective': 'reg:squarederror',\n",
    "          'tree_method': 'gpu_hist',\n",
    "          'booster': 'gbtree',\n",
    "          'base_score': 5,\n",
    "          'learning_rate': 0.05,\n",
    "          'max_depth': 5,\n",
    "          'min_child_weight': 5,\n",
    "          'eval_metric': 'rmse',\n",
    "          'subsample': 1,\n",
    "          'colsample_bytree': 0.8,\n",
    "          'reg_lambda': 1,\n",
    "          'reg_alpha': 0,\n",
    "          'seed': 42,\n",
    "          'nthread': -1\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TCAoOMeV-wuH",
    "outputId": "9e63a9b9-7d5f-40d2-c3c3-1a7a08a1ab20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training model: my_XGBRegressor\n",
      "ROUND 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[0]\ttrain-rmse:10.28633\tval-rmse:10.28486\n",
      "[500]\ttrain-rmse:0.09520\tval-rmse:0.12781\n",
      "[1000]\ttrain-rmse:0.07312\tval-rmse:0.12571\n",
      "[1060]\ttrain-rmse:0.07112\tval-rmse:0.12570\n",
      "[1010]  RMSE: 0.125669\n",
      "[0]\ttrain-rmse:10.28631\n",
      "[500]\ttrain-rmse:0.09851\n",
      "[1000]\ttrain-rmse:0.07861\n",
      "[1009]\ttrain-rmse:0.07831\n",
      "[ALL]  train-RMSE  : 0.07862343721577819\n",
      "[ALL]  holdout-RMSE: 0.1264323584414182\n",
      "ROUND 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[0]\ttrain-rmse:10.28398\tval-rmse:10.28021\n",
      "[500]\ttrain-rmse:0.09415\tval-rmse:0.12778\n",
      "[1000]\ttrain-rmse:0.07334\tval-rmse:0.12602\n",
      "[1055]\ttrain-rmse:0.07181\tval-rmse:0.12599\n",
      "[1006]  RMSE: 0.125976\n",
      "[0]\ttrain-rmse:10.28401\n",
      "[500]\ttrain-rmse:0.09778\n",
      "[1000]\ttrain-rmse:0.07782\n",
      "[1005]\ttrain-rmse:0.07771\n",
      "[ALL]  train-RMSE  : 0.07813928715827942\n",
      "[ALL]  holdout-RMSE: 0.12781656752603515\n",
      "ROUND 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[0]\ttrain-rmse:10.28422\tval-rmse:10.28867\n",
      "[500]\ttrain-rmse:0.09337\tval-rmse:0.13216\n",
      "[1000]\ttrain-rmse:0.07249\tval-rmse:0.13024\n",
      "[1080]\ttrain-rmse:0.07001\tval-rmse:0.13022\n",
      "[1030]  RMSE: 0.130195\n",
      "[0]\ttrain-rmse:10.28420\n",
      "[500]\ttrain-rmse:0.09743\n",
      "[1000]\ttrain-rmse:0.07850\n",
      "[1029]\ttrain-rmse:0.07772\n",
      "[ALL]  train-RMSE  : 0.07872189039868825\n",
      "[ALL]  holdout-RMSE: 0.12666508314935312\n",
      "ROUND 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[0]\ttrain-rmse:10.28738\tval-rmse:10.27849\n",
      "[500]\ttrain-rmse:0.09328\tval-rmse:0.13662\n",
      "[951]\ttrain-rmse:0.07524\tval-rmse:0.13461\n",
      "[901]  RMSE: 0.134592\n",
      "[0]\ttrain-rmse:10.28741\n",
      "[500]\ttrain-rmse:0.09777\n",
      "[900]\ttrain-rmse:0.08239\n",
      "[ALL]  train-RMSE  : 0.08325487722391534\n",
      "[ALL]  holdout-RMSE: 0.12462802152290993\n",
      "ROUND 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[0]\ttrain-rmse:10.28529\tval-rmse:10.28679\n",
      "[500]\ttrain-rmse:0.09434\tval-rmse:0.13156\n",
      "[960]\ttrain-rmse:0.07417\tval-rmse:0.12954\n",
      "[910]  RMSE: 0.12951\n",
      "[0]\ttrain-rmse:10.28524\n",
      "[500]\ttrain-rmse:0.09775\n",
      "[909]\ttrain-rmse:0.08134\n",
      "[ALL]  train-RMSE  : 0.08260074375833687\n",
      "[ALL]  holdout-RMSE: 0.12722517352591903\n",
      "\n",
      "\n",
      "[THE END]  train-RMSE  : 0.1265493086690048\n"
     ]
    }
   ],
   "source": [
    "#stacker\n",
    "LR = LinearRegression()\n",
    "\n",
    "#base models\n",
    "XGB_F = my_XGBRegressor(params)\n",
    "\n",
    "E = Ensemble(\n",
    "    n_folds=5,\n",
    "    stacker=LR,\n",
    "    base_models=[XGB_F]\n",
    ")\n",
    "\n",
    "y_pred = E.fit_predict(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save('predictions/XGB_F_norm_mean_train', E.S_train)\n",
    "np.save('predictions/XGB_F_norm_mean_test', E.S_test)\n",
    "(np.load('predictions/XGB_F_norm_mean_train.npy') == E.S_train).all(), (np.load('predictions/XGB_F_norm_mean_test.npy') == E.S_test).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "UE3unrr2-wuI"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/submits/sample_submission.csv', index_col='id')\n",
    "result = np.expm1(E.S_test)\n",
    "\n",
    "if len(result[result < 0]):\n",
    "    print('WARNING: NEGATIVE PREDICTIONS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "cUD3MPsL-wuI"
   },
   "outputs": [],
   "source": [
    "submission['price_doc'] = 1.02*result # 0.9\n",
    "submission.to_csv('data/submits/submission.csv', index='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "swIVUL0G-wuJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 181k/181k [00:02<00:00, 90.4kB/s]\n",
      "Successfully submitted to Sberbank Russian Housing Market"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c sberbank-russian-housing-market -f \"data/submits/submission.csv\" -m \"!XGB_F norm mean! 1.02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
