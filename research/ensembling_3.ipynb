{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/stasvlad/Documents/hse/sberbank\n"
     ]
    }
   ],
   "source": [
    "%cd /home/stasvlad/Documents/hse/sberbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2TDIoXTN-wtp"
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from features import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBRegressor, DMatrix, cv\n",
    "from xgboost import train as train_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70sx-8YD-wtx"
   },
   "source": [
    "## Data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lgqwWeGS-wtz",
    "outputId": "e10c5ac4-07ff-420c-bfdf-c1e8390d247d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix:  550\n",
      "Fix:  149\n"
     ]
    }
   ],
   "source": [
    "macro_df = pd.read_csv('data/macro.csv', parse_dates=['timestamp'])\n",
    "train_df = pd.read_csv('data/train.csv', index_col='id', parse_dates=['timestamp'])\n",
    "test_df = pd.read_csv('data/test.csv', index_col='id', parse_dates=['timestamp'])\n",
    "\n",
    "tverskoe_issue_fix(train_df)\n",
    "tverskoe_issue_fix(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XMfRzcU-wt2"
   },
   "source": [
    "## 1. Data preprocessing\n",
    "## I part (encoding and correcting mistakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlOmdlGc-wt4"
   },
   "source": [
    "### Macro dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a1-RKD2-wt4",
    "outputId": "d5aa3070-d1f0-48cd-8b8d-7d7b2828b57f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "macro_df['child_on_acc_pre_school'] = macro_df['child_on_acc_pre_school'].str.replace('#!', 'nan')\n",
    "for column in macro_df.select_dtypes('object').columns:\n",
    "    macro_df[column] = macro_df[column].str.replace(',', '.')\n",
    "    macro_df[column] = macro_df[column].astype(float)\n",
    "\n",
    "if not len(macro_df.select_dtypes('object').columns):\n",
    "    print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hOdB2O2-wt5"
   },
   "source": [
    "### Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cefhW3sO-wt5"
   },
   "outputs": [],
   "source": [
    "train_df = encode(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSRp5y_C-wt7"
   },
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IMeNQ_Ad-wt8"
   },
   "outputs": [],
   "source": [
    "test_df = encode(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcRHlWsU-wt-"
   },
   "source": [
    "## II part (Filling missing values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8kHIiU0-wt_"
   },
   "source": [
    "XGBRegressor model handles `np.NaN` values itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfLaI5aH-wt_"
   },
   "source": [
    "## 2. Encoding `sub_area` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MZRCy_uubb-L"
   },
   "outputs": [],
   "source": [
    "coords_train_df = pd.read_csv('data/geo/train_lat_lon.csv')\n",
    "coords_train_df.drop(['key', 'tolerance_m'], axis=1, inplace=True)\n",
    "coords_train_df.index = coords_train_df.id\n",
    "coords_train_df.drop(['id'], axis=1, inplace=True)\n",
    "coords_train_df = coords_train_df.sort_index()\n",
    "\n",
    "coords_test_df = pd.read_csv('data/geo/test_lat_lon.csv')\n",
    "coords_test_df.drop(['key', 'tolerance_m'], axis=1, inplace=True)\n",
    "coords_test_df.index = coords_test_df.id\n",
    "coords_test_df.drop(['id'], axis=1, inplace=True)\n",
    "coords_test_df = coords_test_df.sort_index()\n",
    "\n",
    "coords_all_df = pd.concat([coords_train_df, coords_test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qpehSLDmbb-L"
   },
   "outputs": [],
   "source": [
    "train_df['is_train'] = 1\n",
    "test_df['is_train'] = 0\n",
    "\n",
    "# coords_df = pd.read_csv('data/coords.csv', index_col='id')\n",
    "all_df = pd.concat([train_df, test_df])\n",
    "\n",
    "all_df['latitude'] = coords_all_df['lat']\n",
    "all_df['longitude'] = coords_all_df['lon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ad9TaN9Q-wuB"
   },
   "source": [
    "## 3. Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GS7abrdJ-wuB"
   },
   "outputs": [],
   "source": [
    "all_df = remove_outliers(all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Y7doThf-wuB"
   },
   "source": [
    "## 4. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HEFaFZFI-wuC"
   },
   "outputs": [],
   "source": [
    "all_df = create_new_features(all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLOjaYTj-wuD"
   },
   "source": [
    "## 5. Removing fake prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2gVmPZwh-wuE"
   },
   "outputs": [],
   "source": [
    "train_df = all_df[all_df['is_train'] == 1].drop(['is_train'], axis=1)\n",
    "test_df = all_df[all_df['is_train'] == 0].drop(['is_train', 'price_doc'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QuNretEQbb-O",
    "outputId": "7adc459e-e555-4573-847a-44fc48768b8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVED: 35\n"
     ]
    }
   ],
   "source": [
    "train_df = remove_fake_prices(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "r2iMQl-I-wuE"
   },
   "outputs": [],
   "source": [
    "idx_outliers = np.loadtxt('outliers/idx_outliers_full.txt').astype(int)\n",
    "train_df = train_df.drop(idx_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Z1VhzPO-wuF"
   },
   "source": [
    "### `Ensembling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETR_train = np.load('predictions/ETR_train.npy')\n",
    "ETR_test = np.load('predictions/ETR_test.npy')\n",
    "\n",
    "GBR_train = np.load('predictions/GBR_train.npy')\n",
    "GBR_test = np.load('predictions/GBR_test.npy')\n",
    "\n",
    "LGB_F_train = np.load('predictions/LGB_F_train.npy')\n",
    "LGB_F_test = np.load('predictions/LGB_F_test.npy')\n",
    "\n",
    "RF_train = np.load('predictions/RF_train.npy')\n",
    "RF_test = np.load('predictions/RF_test.npy')\n",
    "\n",
    "XGB_F_norm_mean_train = np.load('predictions/XGB_F_norm_mean_train.npy')\n",
    "XGB_F_norm_mean_test = np.load('predictions/XGB_F_norm_mean_test.npy')\n",
    "\n",
    "XGB_F_norm_median_train = np.load('predictions/XGB_F_norm_median_train.npy')\n",
    "XGB_F_norm_median_test = np.load('predictions/XGB_F_norm_median_test.npy')\n",
    "\n",
    "XGB_F_split_0_train = np.load('predictions/XGB_F_split_0_train.npy')\n",
    "XGB_F_split_0_test = np.load('predictions/XGB_F_split_0_test.npy')\n",
    "\n",
    "XGB_F_split_1_train = np.load('predictions/XGB_F_split_1_train.npy')\n",
    "XGB_F_split_1_test = np.load('predictions/XGB_F_split_1_test.npy')\n",
    "\n",
    "XGB_F_train = np.load('predictions/XGB_F_train.npy')\n",
    "XGB_F_test = np.load('predictions/XGB_F_test.npy')\n",
    "\n",
    "XGB_F_with_weights_train = np.load('predictions/XGB_F_with_weights_train.npy')\n",
    "XGB_F_with_weights_test = np.load('predictions/XGB_F_with_weights_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log1p(train_df['price_doc']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27762, 8), (7662, 8))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_train = np.concatenate(\n",
    "    [ETR_train,\n",
    "     GBR_train,\n",
    "     LGB_F_train,\n",
    "     RF_train,\n",
    "     XGB_F_norm_mean_train,\n",
    "     XGB_F_norm_median_train,\n",
    "     # XGB_F_split_0_train,\n",
    "     # XGB_F_split_1_train,\n",
    "     XGB_F_train,\n",
    "     XGB_F_with_weights_train],\n",
    "    axis=1\n",
    ")\n",
    "S_test = np.concatenate(\n",
    "    [ETR_test,\n",
    "     GBR_test,\n",
    "     LGB_F_test,\n",
    "     RF_test,\n",
    "     XGB_F_norm_mean_test,\n",
    "     XGB_F_norm_median_test,\n",
    "     # XGB_F_split_0_test,\n",
    "     # XGB_F_split_1_test,\n",
    "     XGB_F_test,\n",
    "     XGB_F_with_weights_test],\n",
    "    axis=1)\n",
    "\n",
    "S_train.shape, S_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-RMSE  : 0.12501300072597782\n"
     ]
    }
   ],
   "source": [
    "stacker = LinearRegression(fit_intercept=False)\n",
    "stacker.fit(S_train, y)\n",
    "y_pred = stacker.predict(S_test)\n",
    "y_pred_train = stacker.predict(S_train)\n",
    "print(f\"train-RMSE  : {mean_squared_error(y_pred_train, y, squared=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "UE3unrr2-wuI"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/submits/sample_submission.csv', index_col='id')\n",
    "result = np.expm1(y_pred)\n",
    "\n",
    "if len(result[result < 0]):\n",
    "    print('WARNING: NEGATIVE PREDICTIONS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "cUD3MPsL-wuI"
   },
   "outputs": [],
   "source": [
    "submission['price_doc'] = 0.915*result # 0.9\n",
    "submission.to_csv('data/submits/submission.csv', index='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "swIVUL0G-wuJ",
    "outputId": "11d4dce8-0b10-48d8-b798-a10cdc4894e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 181k/181k [00:02<00:00, 90.3kB/s]\n",
      "Successfully submitted to Sberbank Russian Housing Market"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c sberbank-russian-housing-market -f \"data/submits/submission.csv\" -m \"Ensemble (LR)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_XGBRegressor(object):\n",
    "    def __init__(self, params, product_type=-1):\n",
    "        self.params = params\n",
    "        self.product_type = product_type\n",
    "\n",
    "    def fit(self, X, y, w=None):\n",
    "        # if w == None:\n",
    "        #    w = np.ones(X.shape[0])\n",
    "\n",
    "        if self.product_type == 0:\n",
    "            X = train_df[train_df['product_type'] == 0].drop(['sub_area', 'price_doc'], axis=1).values\n",
    "            y = np.log1p(test_df[test_df['product_type'] == 0]['price_doc'].values)\n",
    "            print(X.shape)\n",
    "\n",
    "        if self.product_type == 1:\n",
    "            X = train_df[train_df['product_type'] == 1].drop(['sub_area', 'price_doc'], axis=1).values\n",
    "            y = np.log1p(test_df[test_df['product_type'] == 1]['price_doc'].values)\n",
    "            print(X.shape)\n",
    "            \n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)  # random_state=42\n",
    "        d_train = DMatrix(X_train, label=y_train)  # weight = w_train\n",
    "        d_valid = DMatrix(X_val, label=y_val)  # weight = w_valid\n",
    "\n",
    "        print(f\"Training until validation scores don't improve for 50 rounds\") # !!!\n",
    "        if self.params['booster'] == 'gblinear':\n",
    "            num_boost_round = 50000\n",
    "        else:\n",
    "            num_boost_round = 5000\n",
    "\n",
    "        bst_partial = train_xgb(self.params,\n",
    "                                d_train,\n",
    "                                num_boost_round=num_boost_round,\n",
    "                                early_stopping_rounds=50,\n",
    "                                evals=[(d_train, 'train'), (d_valid, 'val')],\n",
    "                                verbose_eval=500)\n",
    "\n",
    "        last_round = bst_partial.best_iteration\n",
    "        print(f\"[{last_round}]  RMSE: {bst_partial.best_score}\")\n",
    "\n",
    "        d_all = DMatrix(X, label=y)  # weight = w\n",
    "        self.bst = train_xgb(self.params,\n",
    "                             d_all,\n",
    "                             num_boost_round=last_round,\n",
    "                             evals=[(d_train, 'train')],\n",
    "                             verbose_eval=500)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        d_test = DMatrix(X_test)\n",
    "        return self.bst.predict(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['w'] = 1\n",
    "train_df.loc[train_df['timestamp_year'] == 2014, 'w'] = 1.2\n",
    "train_df.loc[train_df['timestamp_year'] == 2015, 'w'] = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_xgb_lin = {'objective': 'reg:squarederror',\n",
    "                  'booster': 'gblinear',\n",
    "                  'base_score': 7,\n",
    "                  'max_depth': 5,\n",
    "                  'learning_rate': 0.3,\n",
    "                  'eval_metric': 'rmse',\n",
    "                  'seed': 42,\n",
    "                  'nthread': -1\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[00:58:50] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:8.57161\tval-rmse:8.57470\n",
      "[500]\ttrain-rmse:0.23250\tval-rmse:0.22704\n",
      "[1000]\ttrain-rmse:0.21343\tval-rmse:0.20834\n",
      "[1500]\ttrain-rmse:0.19735\tval-rmse:0.19258\n",
      "[2000]\ttrain-rmse:0.18392\tval-rmse:0.17941\n",
      "[2500]\ttrain-rmse:0.17278\tval-rmse:0.16849\n",
      "[3000]\ttrain-rmse:0.16363\tval-rmse:0.15953\n",
      "[3500]\ttrain-rmse:0.15616\tval-rmse:0.15220\n",
      "[4000]\ttrain-rmse:0.15010\tval-rmse:0.14625\n",
      "[4500]\ttrain-rmse:0.14522\tval-rmse:0.14148\n",
      "[5000]\ttrain-rmse:0.14133\tval-rmse:0.13766\n",
      "[5500]\ttrain-rmse:0.13822\tval-rmse:0.13462\n",
      "[6000]\ttrain-rmse:0.13577\tval-rmse:0.13222\n",
      "[6500]\ttrain-rmse:0.13383\tval-rmse:0.13032\n",
      "[7000]\ttrain-rmse:0.13231\tval-rmse:0.12883\n",
      "[7500]\ttrain-rmse:0.13111\tval-rmse:0.12767\n",
      "[8000]\ttrain-rmse:0.13018\tval-rmse:0.12675\n",
      "[8500]\ttrain-rmse:0.12945\tval-rmse:0.12604\n",
      "[9000]\ttrain-rmse:0.12887\tval-rmse:0.12548\n",
      "[9500]\ttrain-rmse:0.12843\tval-rmse:0.12504\n",
      "[10000]\ttrain-rmse:0.12807\tval-rmse:0.12470\n",
      "[10500]\ttrain-rmse:0.12780\tval-rmse:0.12444\n",
      "[11000]\ttrain-rmse:0.12759\tval-rmse:0.12423\n",
      "[11500]\ttrain-rmse:0.12742\tval-rmse:0.12407\n",
      "[12000]\ttrain-rmse:0.12729\tval-rmse:0.12394\n",
      "[12500]\ttrain-rmse:0.12718\tval-rmse:0.12384\n",
      "[13000]\ttrain-rmse:0.12710\tval-rmse:0.12376\n",
      "[13500]\ttrain-rmse:0.12703\tval-rmse:0.12370\n",
      "[14000]\ttrain-rmse:0.12698\tval-rmse:0.12365\n",
      "[14500]\ttrain-rmse:0.12694\tval-rmse:0.12361\n",
      "[15000]\ttrain-rmse:0.12690\tval-rmse:0.12357\n",
      "[15500]\ttrain-rmse:0.12687\tval-rmse:0.12355\n",
      "[16000]\ttrain-rmse:0.12685\tval-rmse:0.12352\n",
      "[16500]\ttrain-rmse:0.12683\tval-rmse:0.12350\n",
      "[17000]\ttrain-rmse:0.12682\tval-rmse:0.12349\n",
      "[17500]\ttrain-rmse:0.12680\tval-rmse:0.12347\n",
      "[18000]\ttrain-rmse:0.12679\tval-rmse:0.12346\n",
      "[18500]\ttrain-rmse:0.12678\tval-rmse:0.12345\n",
      "[18707]\ttrain-rmse:0.12677\tval-rmse:0.12344\n",
      "[18657]  RMSE: 0.123445\n",
      "[00:59:21] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:8.57040\n",
      "[500]\ttrain-rmse:0.23762\n",
      "[1000]\ttrain-rmse:0.21786\n",
      "[1500]\ttrain-rmse:0.20116\n",
      "[2000]\ttrain-rmse:0.18715\n",
      "[2500]\ttrain-rmse:0.17550\n",
      "[3000]\ttrain-rmse:0.16586\n",
      "[3500]\ttrain-rmse:0.15797\n",
      "[4000]\ttrain-rmse:0.15156\n",
      "[4500]\ttrain-rmse:0.14637\n",
      "[5000]\ttrain-rmse:0.14221\n",
      "[5500]\ttrain-rmse:0.13889\n",
      "[6000]\ttrain-rmse:0.13625\n",
      "[6500]\ttrain-rmse:0.13416\n",
      "[7000]\ttrain-rmse:0.13251\n",
      "[7500]\ttrain-rmse:0.13122\n",
      "[8000]\ttrain-rmse:0.13020\n",
      "[8500]\ttrain-rmse:0.12940\n",
      "[9000]\ttrain-rmse:0.12878\n",
      "[9500]\ttrain-rmse:0.12829\n",
      "[10000]\ttrain-rmse:0.12790\n",
      "[10500]\ttrain-rmse:0.12760\n",
      "[11000]\ttrain-rmse:0.12737\n",
      "[11500]\ttrain-rmse:0.12718\n",
      "[12000]\ttrain-rmse:0.12704\n",
      "[12500]\ttrain-rmse:0.12692\n",
      "[13000]\ttrain-rmse:0.12683\n",
      "[13500]\ttrain-rmse:0.12676\n",
      "[14000]\ttrain-rmse:0.12671\n",
      "[14500]\ttrain-rmse:0.12666\n",
      "[15000]\ttrain-rmse:0.12662\n",
      "[15500]\ttrain-rmse:0.12659\n",
      "[16000]\ttrain-rmse:0.12657\n",
      "[16500]\ttrain-rmse:0.12655\n",
      "[17000]\ttrain-rmse:0.12653\n",
      "[17500]\ttrain-rmse:0.12651\n",
      "[18000]\ttrain-rmse:0.12650\n",
      "[18500]\ttrain-rmse:0.12649\n",
      "[18656]\ttrain-rmse:0.12649\n",
      "train-RMSE  : 0.12581600295575499\n"
     ]
    }
   ],
   "source": [
    "stacker = my_XGBRegressor(params_xgb_lin)\n",
    "stacker.fit(S_train, y, train_df['w'])\n",
    "y_pred = stacker.predict(S_test)\n",
    "y_pred_train = stacker.predict(S_train)\n",
    "print(f\"train-RMSE  : {mean_squared_error(y_pred_train, y, squared=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "UE3unrr2-wuI"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/submits/sample_submission.csv', index_col='id')\n",
    "result = np.expm1(y_pred)\n",
    "\n",
    "if len(result[result < 0]):\n",
    "    print('WARNING: NEGATIVE PREDICTIONS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "cUD3MPsL-wuI"
   },
   "outputs": [],
   "source": [
    "submission['price_doc'] = 0.935*result # 0.9\n",
    "submission.to_csv('data/submits/submission.csv', index='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 121k/121k [00:02<00:00, 60.5kB/s]\n",
      "Successfully submitted to Sberbank Russian Housing Market"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c sberbank-russian-housing-market -f \"data/submits/submission.csv\" -m \"Ensemble\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
