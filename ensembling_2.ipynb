{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBRegressor, DMatrix, cv\n",
    "from xgboost import train as train_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix:  550\n",
      "Fix:  149\n"
     ]
    }
   ],
   "source": [
    "macro_df = pd.read_csv('data/macro.csv', parse_dates=['timestamp'])\n",
    "train_df = pd.read_csv('data/train.csv', index_col='id', parse_dates=['timestamp'])\n",
    "test_df = pd.read_csv('data/test.csv', index_col='id', parse_dates=['timestamp'])\n",
    "tverskoe_issue_fix(train_df)\n",
    "tverskoe_issue_fix(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing\n",
    "## I part (encoding and correcting mistakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macro dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "macro_df['child_on_acc_pre_school'] = macro_df['child_on_acc_pre_school'].str.replace('#!', 'nan')\n",
    "for column in macro_df.select_dtypes('object').columns:\n",
    "    macro_df[column] = macro_df[column].str.replace(',', '.')\n",
    "    macro_df[column] = macro_df[column].astype(float)\n",
    "\n",
    "if not len(macro_df.select_dtypes('object').columns):\n",
    "    print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = encode(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = encode(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II part (Filling missing values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBRegressor model handles `np.NaN` values itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoding `sub_area` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['is_train'] = 1\n",
    "test_df['is_train'] = 0\n",
    "\n",
    "coords_df = pd.read_csv('data/coords.csv', index_col='id')\n",
    "all_df = pd.concat([train_df, test_df])\n",
    "\n",
    "all_df['latitude'] = coords_df['latitude']\n",
    "all_df['longitude'] = coords_df['longitude']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = remove_outliers(all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = create_new_features(all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Removing fake prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = all_df[all_df['is_train'] == 1].drop(['is_train'], axis=1)\n",
    "test_df = all_df[all_df['is_train'] == 0].drop(['is_train', 'price_doc'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = remove_fake_prices(train_df)\n",
    "idx_outliers = np.loadtxt('data/idx_outliers.txt').astype(int)\n",
    "train_df = train_df.drop(idx_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Ensembling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_LGBRegressor(object):\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def fit(self, X, y, w=None):\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)  # random_state=42\n",
    "        # x_train, y_train, w_train, x_valid, y_valid,  w_valid = X[train_id], y[train_id], w[train_id], X[test_id], y[test_id], w[test_id],\n",
    "        d_train = lgb.Dataset(X_train, y_train)  # weight=w_train\n",
    "        d_valid = lgb.Dataset(X_val, y_val)  # weight=w_val\n",
    "\n",
    "        bst_partial = lgb.train(self.params,\n",
    "                                d_train, 10000,\n",
    "                                valid_sets=d_valid,\n",
    "                                callbacks = [lgb.early_stopping(50)])\n",
    "                                \n",
    "        num_round = bst_partial.best_iteration\n",
    "        d_all = lgb.Dataset(X, label=y)  # weight=w\n",
    "        self.bst = lgb.train(self.params, d_all, num_round)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.bst.predict(X)\n",
    "\n",
    "\n",
    "class my_XGBRegressor(object):\n",
    "    def __init__(self, params, product_type=-1):\n",
    "        self.params = params\n",
    "        self.product_type = product_type\n",
    "\n",
    "    def fit(self, X, y, w=None):\n",
    "        # if w == None:\n",
    "        #    w = np.ones(X.shape[0])\n",
    "\n",
    "        if self.product_type == 0:\n",
    "            X = train_df[train_df['product_type'] == 0].drop(['sub_area', 'price_doc'], axis=1).values\n",
    "            y = test_df[test_df['product_type'] == 0]['price_doc'].values\n",
    "            print(X.shape)\n",
    "\n",
    "        if self.product_type == 1:\n",
    "            X = train_df[train_df['product_type'] == 1].drop(['sub_area', 'price_doc'], axis=1).values\n",
    "            y = test_df[test_df['product_type'] == 1]['price_doc'].values\n",
    "            print(X.shape)\n",
    "            \n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)  # random_state=42\n",
    "        d_train = DMatrix(X_train, label=y_train)  # weight = w_train\n",
    "        d_valid = DMatrix(X_val, label=y_val)  # weight = w_valid\n",
    "\n",
    "        print(f\"Training until validation scores don't improve for 50 rounds\") # !!!\n",
    "\n",
    "        bst_partial = train_xgb(self.params,\n",
    "                                d_train,\n",
    "                                num_boost_round=5000,\n",
    "                                early_stopping_rounds=50,\n",
    "                                evals=[(d_train, 'train'), (d_valid, 'val')],\n",
    "                                verbose_eval=False)\n",
    "\n",
    "        last_round = bst_partial.best_iteration\n",
    "        print(f\"[{last_round}]  RMSE: {bst_partial.best_score}\")\n",
    "\n",
    "        d_all = DMatrix(X, label=y)  # weight = w\n",
    "        self.bst = train_xgb(self.params,\n",
    "                             d_all,\n",
    "                             num_boost_round=last_round,\n",
    "                             evals=[(d_train, 'train')],\n",
    "                             verbose_eval=False)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        d_test = DMatrix(X_test)\n",
    "        return self.bst.predict(d_test)\n",
    "\n",
    "\n",
    "class Ensemble(object):\n",
    "    def __init__(self, n_folds, stacker, base_models):\n",
    "        self.n_folds = n_folds\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit_predict(self, train_df, test_df):\n",
    "        X = train_df.drop(['sub_area', 'price_doc'], axis=1).values\n",
    "        y = np.log1p(train_df['price_doc']).values\n",
    "        # w = train_df['w'].values\n",
    "        X_test = test_df.drop('sub_area', axis=1).values\n",
    "\n",
    "        all_df = pd.concat([train_df.drop(['sub_area', 'price_doc'], axis=1), test_df.drop('sub_area', axis=1)])\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        imputer.fit(all_df)\n",
    "\n",
    "        kf = KFold(n_splits=self.n_folds, shuffle=True)  # random_state=42\n",
    "        folds = list(kf.split(X, y))\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test = np.zeros((X_test.shape[0], len(self.base_models)))\n",
    "\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            print('\\n\\nTraining model: ' + str(type(model).__name__))\n",
    "            S_test_i = np.zeros((X_test.shape[0], len(folds)))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                print('ROUND ' + str(j+1))\n",
    "\n",
    "                if (not isinstance(model, my_XGBRegressor)) and (not isinstance(model, my_LGBRegressor)):\n",
    "                    X = imputer.transform(X)\n",
    "                    X_test = imputer.transform(X_test)\n",
    "\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                # w_train = w[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                y_holdout = y[test_idx]\n",
    "\n",
    "                model.fit(X_train, y_train)  # w_train\n",
    "\n",
    "                y_train_pred = model.predict(X_train)\n",
    "                y_pred = model.predict(X_holdout)\n",
    "\n",
    "                print(f\"[ALL]  train-RMSE  : {mean_squared_error(y_train_pred, y_train, squared=False)}\")\n",
    "                print(f\"[ALL]  holdout-RMSE: {mean_squared_error(y_pred, y_holdout, squared=False)}\")\n",
    "\n",
    "                S_train[test_idx, i] = y_pred\n",
    "                S_test_i[:, j] = model.predict(X_test)\n",
    "\n",
    "            S_test[:, i] = S_test_i.mean(axis=1)\n",
    "\n",
    "        self.stacker.fit(S_train, y)\n",
    "        y_pred = self.stacker.predict(S_test)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(['sub_area', 'price_doc'], axis=1).copy()\n",
    "y = np.log1p(train_df['price_doc'])\n",
    "X_test = test_df.drop(['sub_area'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_xgb_tree = {'objective': 'reg:squarederror',\n",
    "                   'booster': 'gbtree',\n",
    "                   'tree_method': 'gpu_hist',\n",
    "                   'base_score': 7,\n",
    "                   'learning_rate': 0.05,\n",
    "                   'max_depth': 4,\n",
    "                   'min_child_weight': 7,\n",
    "                   'subsample': 1,\n",
    "                   'colsample_bytree': 0.9,\n",
    "                   'reg_lambda': 5,\n",
    "                   'reg_alpha': 1,\n",
    "                   'eval_metric': 'rmse',\n",
    "                   'seed': 42,\n",
    "                   'nthread': -1\n",
    "                   }\n",
    "\n",
    "\n",
    "params_xgb_lin = {'objective': 'reg:squarederror',\n",
    "                  'booster': 'gblinear',\n",
    "                  'tree_method': 'gpu_hist',\n",
    "                  'n_estimators': 500,\n",
    "                  'base_score': 7,\n",
    "                  'learning_rate': 0.05,\n",
    "                  'alpha':0,\n",
    "                  'eval_metric': 'rmse',\n",
    "                  'seed': 42,\n",
    "                  'nthread': -1\n",
    "                  }\n",
    "\n",
    "params_lgb = {'objective': 'regression', \n",
    "              'metric': 'rmse',\n",
    "              'learning_rate': 0.05, \n",
    "              'max_depth': -1, \n",
    "              'sub_feature': 0.7, \n",
    "              'sub_row': 0.9,\n",
    "              'num_leaves': 15, \n",
    "              'min_data': 30, \n",
    "              'max_bin': 20,\n",
    "              'bagging_freq': 40,\n",
    "              'force_col_wise': True,\n",
    "              'verbosity': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training model: my_LGBRegressor\n",
      "ROUND 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1208]\tvalid_0's rmse: 0.135057\n",
      "[ALL]  train-RMSE: 0.09268721831168733\n",
      "[ALL]  holdout-RMSE: 0.13481935791160235\n",
      "ROUND 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[755]\tvalid_0's rmse: 0.139617\n",
      "[ALL]  train-RMSE: 0.10506384845299857\n",
      "[ALL]  holdout-RMSE: 0.13462913317102632\n",
      "ROUND 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1478]\tvalid_0's rmse: 0.135332\n",
      "[ALL]  train-RMSE: 0.0867151819405843\n",
      "[ALL]  holdout-RMSE: 0.13053927227289805\n",
      "ROUND 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1243]\tvalid_0's rmse: 0.13006\n",
      "[ALL]  train-RMSE: 0.0910278202792408\n",
      "[ALL]  holdout-RMSE: 0.13093370009762667\n",
      "ROUND 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1074]\tvalid_0's rmse: 0.126201\n",
      "[ALL]  train-RMSE: 0.09557425831981475\n",
      "[ALL]  holdout-RMSE: 0.12877454606151142\n",
      "\n",
      "\n",
      "Training model: my_XGBRegressor\n",
      "ROUND 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[1483]  RMSE: 0.126904\n",
      "[ALL]  train-RMSE: 0.09117716481599018\n",
      "[ALL]  holdout-RMSE: 0.1286235334333339\n",
      "ROUND 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[1967]  RMSE: 0.132457\n",
      "[ALL]  train-RMSE: 0.08418121558663577\n",
      "[ALL]  holdout-RMSE: 0.12833264346547313\n",
      "ROUND 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[1747]  RMSE: 0.130243\n",
      "[ALL]  train-RMSE: 0.0873594500625978\n",
      "[ALL]  holdout-RMSE: 0.12651221263782927\n",
      "ROUND 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[2223]  RMSE: 0.130641\n",
      "[ALL]  train-RMSE: 0.08151459840090393\n",
      "[ALL]  holdout-RMSE: 0.12789148600246134\n",
      "ROUND 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[1759]  RMSE: 0.13312\n",
      "[ALL]  train-RMSE: 0.08701678976504279\n",
      "[ALL]  holdout-RMSE: 0.12556585878261384\n",
      "\n",
      "\n",
      "Training model: RandomForestRegressor\n",
      "ROUND 1\n",
      "[ALL]  train-RMSE: 0.20125763752956502\n",
      "[ALL]  holdout-RMSE: 0.20694212747312543\n",
      "ROUND 2\n",
      "[ALL]  train-RMSE: 0.20141694038071226\n",
      "[ALL]  holdout-RMSE: 0.20674110092100054\n",
      "ROUND 3\n",
      "[ALL]  train-RMSE: 0.20254899421963588\n",
      "[ALL]  holdout-RMSE: 0.20781415706912165\n",
      "ROUND 4\n",
      "[ALL]  train-RMSE: 0.20246753094164482\n",
      "[ALL]  holdout-RMSE: 0.20802827031339405\n",
      "ROUND 5\n",
      "[ALL]  train-RMSE: 0.20310859996780187\n",
      "[ALL]  holdout-RMSE: 0.19835299238519546\n",
      "\n",
      "\n",
      "Training model: ExtraTreesRegressor\n",
      "ROUND 1\n",
      "[ALL]  train-RMSE: 0.26152101308360903\n",
      "[ALL]  holdout-RMSE: 0.2648744124713894\n",
      "ROUND 2\n",
      "[ALL]  train-RMSE: 0.2515068908508283\n",
      "[ALL]  holdout-RMSE: 0.2538842475832804\n",
      "ROUND 3\n",
      "[ALL]  train-RMSE: 0.2616328912859339\n",
      "[ALL]  holdout-RMSE: 0.2681626290735557\n",
      "ROUND 4\n",
      "[ALL]  train-RMSE: 0.2633261064914865\n",
      "[ALL]  holdout-RMSE: 0.2662805752218286\n",
      "ROUND 5\n",
      "[ALL]  train-RMSE: 0.26418109077967894\n",
      "[ALL]  holdout-RMSE: 0.2597695049620232\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[04:44:53] ../src/gbm/gbm.cc:26: Unknown gbm type gb_linear\nStack trace:\n  [bt] (0) /home/stasvlad/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f1b851f533f]\n  [bt] (1) /home/stasvlad/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x183f1e) [0x7f1b852e7f1e]\n  [bt] (2) /home/stasvlad/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1d260e) [0x7f1b8533660e]\n  [bt] (3) /home/stasvlad/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterBoostedRounds+0x1a) [0x7f1b851e339a]\n  [bt] (4) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7f1c18a19ff5]\n  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7f1c18a1940a]\n  [bt] (6) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7f1c18766306]\n  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7f1c187669dc]\n  [bt] (8) /bin/python(_PyObject_MakeTpCall+0x29e) [0x5f698e]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16436/1109669774.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_16436/1905797707.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, train_df, test_df)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mS_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS_test_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstacker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstacker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16436/1905797707.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, w)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training until validation scores don't improve for 50 rounds\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# !!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         bst_partial = train_xgb(self.params,\n\u001b[0m\u001b[1;32m     50\u001b[0m                                 \u001b[0md_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                                 \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mBooster\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \"\"\"\n\u001b[0;32m--> 189\u001b[0;31m     bst = _train_internal(params, dtrain,\n\u001b[0m\u001b[1;32m    190\u001b[0m                           \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                           \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     74\u001b[0m             show_stdv=False, cvfolds=None)\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mbst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/xgboost/callback.py\u001b[0m in \u001b[0;36mbefore_training\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;34m'''Function called before training.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'before_training should return the model'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/xgboost/callback.py\u001b[0m in \u001b[0;36mbefore_training\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbefore_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarting_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_boosted_rounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mnum_boosted_rounds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mrounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterBoostedRounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrounds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \"\"\"\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [04:44:53] ../src/gbm/gbm.cc:26: Unknown gbm type gb_linear\nStack trace:\n  [bt] (0) /home/stasvlad/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x9133f) [0x7f1b851f533f]\n  [bt] (1) /home/stasvlad/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x183f1e) [0x7f1b852e7f1e]\n  [bt] (2) /home/stasvlad/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1d260e) [0x7f1b8533660e]\n  [bt] (3) /home/stasvlad/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterBoostedRounds+0x1a) [0x7f1b851e339a]\n  [bt] (4) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7f1c18a19ff5]\n  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7f1c18a1940a]\n  [bt] (6) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7f1c18766306]\n  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7f1c187669dc]\n  [bt] (8) /bin/python(_PyObject_MakeTpCall+0x29e) [0x5f698e]\n\n"
     ]
    }
   ],
   "source": [
    "#stacker\n",
    "xgb_lin = my_XGBRegressor(params_xgb_lin)\n",
    "# LR = LinearRegression()\n",
    "\n",
    "#base models\n",
    "xgb_tree = my_XGBRegressor(params_xgb_tree)\n",
    "\n",
    "xgb_tree_0 = my_XGBRegressor(params_xgb_tree, 0)\n",
    "xgb_tree_1 = my_XGBRegressor(params_xgb_tree, 1)\n",
    "\n",
    "lgb_tree = my_LGBRegressor(params_lgb)\n",
    "\n",
    "RF = RandomForestRegressor(n_estimators=500, max_depth=5, max_features=0.2, n_jobs=-1)\n",
    "ETR = ExtraTreesRegressor(n_estimators=500, max_depth=5, max_features=0.3, n_jobs=-1)\n",
    "Ada = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=5), n_estimators=200)\n",
    "GBR = GradientBoostingRegressor(n_estimators=200, max_depth=5, max_features=0.5)\n",
    "\n",
    "E = Ensemble(\n",
    "    n_folds=5,\n",
    "    stacker=xgb_lin,\n",
    "    base_models=[lgb_tree, xgb_tree, RF, ETR] # -Ada? -GBR? +xgb_tree_0? +xgb_tree_1?\n",
    ")\n",
    "\n",
    "y_pred = E.fit_predict(train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/sample_submission.csv', index_col='id')\n",
    "y_pred = np.expm1(y_pred)\n",
    "\n",
    "if len(y_pred):\n",
    "    print('WARNING: NEGATIVE PREDICTIONS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['price_doc'] = y_pred # 0.9\n",
    "submission.to_csv('submits/submission.csv', index='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 121k/121k [00:02<00:00, 61.0kB/s]\n",
      "Successfully submitted to Sberbank Russian Housing Market"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c sberbank-russian-housing-market -f \"submits/submission.csv\" -m \"Ensemble\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
